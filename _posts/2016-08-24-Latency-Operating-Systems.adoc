= Latency : Operating Systems
:published_at: 2016-08-24
:hp-tags: jHiccup, Latency, Sleep, Operating System, Windows, OSX, Ubuntu, Scientific Linux, Real-Time, Control

//NOTE: Keep X in Mind
//image::cover-image.jpg[150, 250, link="http://docs.hebi.us"]
//video::KCylB780zSM[youtube]

// Writer's guide
// http://asciidoctor.org/docs/asciidoc-writers-guide/#links-and-images
// https://github.com/HubPress/hubpress.io/blob/master/Writers_Guide.adoc

== Introduction

NOTE: TODO: should this blog be about real-time control or on latency with a focus on robotics (motivation section)?

// Arbitrary requirements are bad. Not much information out there. Planning on blog series about various aspects.

Robotic systems tend to be controlled in _real-time_, which means that a command gets executed within a _deadline_ (fixed period of time). There are _hard real-time_ systems that can never exceed the given deadline, and _soft real-time_ systems that are able to occasionally handle reasonable outliers.
 
Although there is a lot of information on the theoretial definition of these terms, it can be challenging to determine reasonable deadlines for a practical application. This is especially true for research environments that build unique mechanisms.

Unfortunately, the control scheme is a systemic concern that can impact the entire system architecture. This can be very difficult to change later on, so there is a tendency towards high requirements with regards to determinism. However, requirements that are too high can result in significant development efforts that may not yield any benefits in the real world.

There is also a lot of folklore about the reliablity and performance of various components. I have been in countless debates where people have tried to convince me that a proposed system has no chance of working, not knowing that it had already been implemented and been running without issues. This is partly because there is very little useful data on latency out there. Most benchmarks focus only on throughput, and some that do include latency were measured incorrectly.

Over a series of blog posts, I'll try to share some my own experiences and data sets. Today's post will focus on operating systems.

=== Measuring Latency

// Data is not normally distributed. What is a better way to look at latency? What are tools that do this? How does jHiccup work? Gil Tene mentions coordinated omission, but that is less of a problem for request/response systems.

The first important realization when looking at latency is that data is not normally (Gaussian) distributed. I have seen many data sets where the worst observed case was more than 1000 standard deviations away from the mean. Looking at only the mean and standard deviation tends to provide an extremely optimistic view that can be misleading.

A better way to look at latency is via histograms and percentile plots, e.g., 99.9% of measurements have been below Xms. Since there are already several blog posts and videos about recording latency, I'd encourage you to refer to the links below for more information,

link:http://psy-lob-saw.blogspot.com/2015/02/hdrhistogram-better-latency-capture.htm[Nitsan Wakart]

link:http://latencytipoftheday.blogspot.com/[Gil Tene]

//video::lJ8ydIuPFeU[youtube]

== Operating Systems

//jHiccup is a great tool developed by Azul Systems that allows us to measure and record hiccups ('jitter')  at the OS level. These can be caused by a large number of reasons, including swap, indexing tasks, and many more. By running it on an idle system, we can measure the best case scenario.
 
The operating system is the base of everything. No matter how amazing the software stack is, the system is fundamentally bound by the capabilities of the OS, it's scheduler, and the overall load on the system.
 
There is a trade-off between responding in a timely manner and overall performance, battery life, and many other concerns. As a result, the major consumer operating systems don't guarantee to meet hard deadlines and can theoretically have arbitrarily long pauses.

However, before dismissing consumer OS for real-time control systems, it is worth looking at their actual performance. Even though there are no theoretical guarantees, the practical performance may be good enough for many applications.

=== Benchmark Setup

link:https://www.azul.com[Azul Systems] sells products targeted at latency critical applications and they have created a variety of useful tools to measure latency. link:https://www.azul.com/jhiccup/[jHiccup] is a tool that allows us to measure and record hiccups (~jitter) at the OS level. By running it on an idle system, we can measure the best case scenario. It uses link:https://github.com/HdrHistogram/HdrHistogram[HdrHistogram] for the actual recording of samples. Lastly, I'm using link:https://github.com/ennerf/HdrHistogramVisualizer[HdrHistogramVisualizer] which is a tool that I've written for visualizing the recorded data.

Hiccups can be caused by a large number of reasons, including scheduling, paging, indexing, and many more. jHiccup measures the time for sleep(1ms) and records the delta to the fastest previously recorded sample. For example, if the fastest sample was 1ms, but it took 3ms to wake up, it will record a 2ms hiccup value.

I've setup two standard desktop computers, one for Windows/Linux and one for Mac tests.

Windows/Linux:: Gigabyte Brix BXi7-4770R, i7-4770R @ 3.2 GHz, 16 GB 1600 MHz DDR3
Mac:: Mac Mini 2014, i7-3720QM @ 2.6 GHz, 16 GB 1600 MHz DDR3

=== Windows / Mac / Linux

Let's first look at the performance of consumer operating systems: Windows, Mac and Linux. Each test started off with a clean install for each OS. The only two modifications to the stock installation were to disable sleep mode and to install JDK8 (update 101) to run jHiccup. I then started the test, unplugged all external cables and let the computer sit 'idle' for >24 hours.

NOTE: TODO: add kernel and exact update versions

//[width="80%",frame="topbot"]
//|=========
//| |*Release* |*Version* |*Update*
//|*Windows* |10 |Enterprise |?
//|*Mac* |9.x |? |?
//|*Linux* |Ubuntu 16.04 |Desktop |?
//|=========

Mac:: OS X, version 10.9.5
Windows:: Windows 10 Enterprise, version 1511 (OS build: 10586.545)
Linux:: Ubuntu 16.04 Desktop, kernel 4.4.0-31-generic
SCL:: Scientific Linux 6.6, kernel 3.10.0-327.rt56.194.el6rt.x86_64

The charts below are split into two sections. The top one shows the worst hiccup that occured within given time intervals. The bottom one shows the percentiles of all measurements across the entire duration.

// 24 hour plot: -/+ 20 min on each side to avoid start/stop noise => sec 1200 to 87600 in aggregate 180 intervals
// 10 min plot: 36005 to 36590 in aggregate 1 intervals

image::os/osx-win-ubuntu_24h.png[24 hour latency plot ]
image::os/osx-win-ubuntu-scl_24h.png[24 hour latency plot ]

Up to the 90th percentile all three systems respond relatively 
similarly, but 

[width="80%"]
|========
| |*Samples* |*Mean* [ms] |*StdDev* |*Max* [ms]
|*Windows 10* |80.304.595 |0.55 |0.37 |17.17
|*OSX 10.9.5*     |65.282.969 |0.32 |0.03 |12.65
|*Ubuntu 16.04*   |78.039.162 |0.10 |0.01 |3.03
|*Scientific Linux 6.6-rt*   |79.753.643 |0.08 |0.01 |0.15
|========

[latex]
---------------
\sigma = 2
---------------

And zoomed in

image::os/osx-win-ubuntu_10m.png[10 minute latency plot]

==== Results (24 hour view)

24 hours in 3 minute intervals. What can create the spikes? How bad is it?

==== Results (10 minute view)

10 minute with 1 second intervals. What can we see in a lower time frame?

=== Linux with RT_PREEMPT

Untuned systems aren't good enough for 1KHz control. Describe Linux setup: SCL6, RT kernel, basic optimizations, highest priority.

It still falls flat.

==	Conclusion

It's easy to do 100 Hz control in just about any OS. 1KHz hard real-time requires lots of tuning. Rates are highly dependent on the application. Hard to generalize.
