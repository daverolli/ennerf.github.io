= A Practical Look at Latency in Robotics : Ethernet and UDP
:published_at: 2016-11-05
:hp-tags: Latency, Ethernet, UDP
:imagesdir: ../images

== Intro

The User Datagram Protocol (UDP) is unreliable, packets may be lost, reordered, and could even be delayed by an arbitrarily long time. These shortcomings make standard UDP seem non-deterministic and a really bad choice for controlling real-time systems. Luckily modern network components have largely solved these issues and performance in practice can be surprisingly good.

Early networks were built using 'Hubs' that share a single channel for all network ports. This results in a large https://en.wikipedia.org/wiki/Collision_domain[Collision Domain] that can cause packet loss whenever more than one sender transmits a packet at the same time. Nowadays Hubs have been replaced with 'Switches' that have dedicated connections for each port in order to isolate collision domains. Connections also tend to be full-duplex, which means that there are dedicated lines for transmitting and receiving data. Since all communications are done over dedicated lines, collisions on the physical (wire) level should be a thing of the past.

Virtually all consumer switches today use the https://en.wikipedia.org/wiki/Store_and_forward[Store-and-Forward] switching architecture. They fully receive the packet, store them in an internal buffer, and then forward them to the appropriate receiver port. This adds a latency cost that scales linearly with the number of switches that a packet has to go through. There is an alternative approach called https://en.wikipedia.org/wiki/Cut-through_switching[Cut-through] switching that can forward packets while they are being received. While this can theoretically lower latency, it is not very popular because it's more difficult to implement and only works well if both ports negotiate the same speed and the receiver port isn't in use. There is also still a requirement to buffer enough of a packet to evaluate the target address in the header, so the benefits primarily effect larger packets.

// Ethercat has solved this in a somewhat elegant way by not using device addresses and by limiting each wire to a single writer. 


// Depending on the use case this can have positive impacts on latency. However, it still requires enough buffering to evaluate the header address and only works well if both sender and receiver ports negotiate the same speed and the sender port isn't already being used.

// In the common https://en.wikipedia.org/wiki/Store_and_forward[Store-and-Forward] switching architecture all packets get buffered by the Switch and are then forwarded to the appropriate receiver port. This adds a latency cost that scales linearly with the number of switches that a packet has to go through. The alternative https://en.wikipedia.org/wiki/Cut-through_switching[Cut-through] switching approach can forward packets before they are fully received, which can have positive impacts on latency especially with very large packets.

<TODO: combine these concepts somehow>

//When more than one component try to communicate, there may be a collision that causes packets to be dropped. Modern networks use 'Switches' that have dedicated connections for each port to isolate collision domains. Additionally, connections tend to be full-duplex, so there are dedicated lines for transmitting and receiving data. Depending on the architecture, packets get buffered and sent to the appropriate port (Store-and-Forward) or directly passed through without evaluation (Cut-Through). Thus, switched networks should never have any packets that get dropped due to collisions. Note that packets can still be lost due to buffer overflows and/or too much load on a system. There is also a latency cost associated with each 'hop' (or buffer) that a packet has to go through.

// re-read
// http://www.cisco.com/c/en/us/products/collateral/switches/nexus-5020-switch/white_paper_c11-465436.html
// https://www.lantronix.com/resources/networking-tutorials/network-switching-tutorial/

Similarly, reordering (in the sense that a stream of packets coming from a single source can arrive out of order) may be relevant for internet communications, but generally does not apply to local networks without redundant routes and load balancing. Depending on the driver implementation it can theoretically happen on a local network, but I have yet to observe a single case.

There are a number of competing networking standards that originated in industrial automation that are built on top of standard Ethernet. They are able to guarantee certain levels of determinism by enforcing tight control over the network layout and by limiting the components that can be connected. However, controlling standard networks in a similar manner can also produce quite good results.

== Benchmark Setup

In order to mitigate jitter caused by the operating system and it's device drivers, I've tried to setup a benchmark that measures the communications latency between two dedicated embedded systems.

1. Sender sends packet to receiver in fixed intervals
2. Receiver echoes packet back to sender
3. Sender measures round trip time and sends results to logging server
4. Logging server persists data to disk

As the sender and receiver devices I'm using two HEBI Robotics I/O Boards. It's a product that we haven't advertised yet, but customers usually use it to integrate actuators with external devices or to get various sensor input into MATLAB. It has 48 pins that serve a variety of functions (analog/digitial I/O, PWM, Encoder input, etc.) that can be accessed remotely via network. It sports a STM32f407 microcontroller and a 100 Mbit network port. I programmed custom firmware to test the network stack. The relevant code pieces can be found https://gist.github.com/ennerf/36a57d432bcff20a58efcdee10f91bd9[here].

image::udp/io-boards.jpg[HEBI Robotics IO Board]

Since the logs are too large to be stored directly on the device, I've setup a https://gist.github.com/ennerf/0ddc4396d15852d28e4eca4a8a923eb7[UDP server (Java)] that receives measurement data and persists the data to disk. The main socket handler writes incoming packets into a double buffered structure that can be persisted by a background thread without halting the packet handler. The synchronization between the threads is done using a http://stuff-gil-says.blogspot.com/2014/11/writerreaderphaser-story-about-new.html[WriterReaderPhaser], which is a synchronization primitive that is very useful for persisting events that are represented by a small amount of data.

The resulting binary data was loaded into MATLAB(C) for analysis and to generate the plots.

== Ethernet Packet Structure

Data packets on the wire include various types of overhead in addition to the actual payload. The minimum overhead is 66 bytes. Additionally, Ethernet Frames have a minimum size of 84 bytes, which makes the minimum payload for a UDP datagram 18 bytes.

.UDP/IP Packet Size
[width="100%",options="header,footer",cols="a,a,a"]
|====================
| Protocol | Item | Size (byte) 

| https://en.wikipedia.org/wiki/Ethernet_frame[Ethernet II] | Preamble | 7
|  | Start of Frame Delimiter | 1
|  | MAC Header + CRC checksum | 18 (no optional fields)
|  | Interpacket gap | 12

| https://en.wikipedia.org/wiki/IPv4[Internet Protocol (IPv4)] | IP Header | 20  (no optional fields)


| https://en.wikipedia.org/wiki/User_Datagram_Protocol[User Datagram Protocol (UDP)] | UDP Header | 8

|====================

All of the benchmarks in this post measure the round-trip time for a UDP datagram with a payload of 200 bytes. After including all overhead, the actual frame size on the wire is 266 bytes. This may seem high to users to are used to dealing with serial protocols, but is representative of our standard use cases. For comparison, the feedback packets of HEBI Robotic's X-Series actuators currently require a payload of 185-215 bytes. Requesting the state of all pins of an I/O board currently requires about 300 bytes.

The theoretical time to send 266 bytes over the wire is 20.3us over 100 Mbit/s Ethernet and 2.03us over 1 Gbit/s Ethernet.

Note that the round trip times in practice are slightly faster because the outgoing packets (commands) are usually significantly smaller than response packets (feedback).

== Baseline: Single Switch

We can establish a baseline of the best-case round trip time (RTT) by having two devices communicate with each other through a single switch that does not see any external traffic. It's possible to create a point-to-point connection without any switches, but the typical use case goes through at least one switch.

I set the frequency to 100Hz and logged data for close to 24 hours. I chose this frequency because it is a common control rate for sending high-level trajectories, and because 10ms is a safe deadline in case there are large outliers. Faster control rates are of course possible.

First, let's look at the jitter of the underlying embedded real-time operating system. The figure below shows the difference between an idealized signal that ticks every 10ms and the actual measurements of the loop start times. 99% are within the lowest measurement resolution (250ns), and the worst observed case is below 6us. This is quite a bit better than the 150us worst observed case on real-time Linux as setup in  https://ennerf.github.io/2016/09/20/A-Practical-Look-at-Latency-in-Robotics-The-Importance-of-Metrics-and-Operating-Systems.html[The Importance of Metrics and Operating Systems].

[.text-center]
.OS jitter of ChibiOS 2.6.8 on STM32F407 (24h)
image::udp/os-jitter-embedded.png[link="/images/udp/os-jitter-embedded.png"]

The figure below shows the round trip time for all packets and the corresponding percentile distribution. There were a total of 8.5 million messages. Note that none of them were lost and that none of them arrived out of order.

[[img-rtt-24h]]
[.text-center]
.Round Trip Time for 200 byte payload (24h)
image::udp/rtt-baseline.png[]

90% of all packets arrive within 194us and a jitter of less than 1 microsecond. Roughly 80us of this time is spent on the wire, so using chips that support Gigabit (rather than 100Mbit) could lower the round trip time to ~120us. Above the common case, there are three different periodically reoccuring modes that cause the round-trip-time to get up to a worst case that is 60us higher. 

* Mode 1 occurs consistently every 5.3 minutes and lasts for 15.01 seconds. During this time it adds up to 4 us latency.
* Mode 2 occurs exactly once every 5 seconds and is always at 210us.
* Mode 3 occurs roughly once an hour and adds linearly increasing latency up to 60us to 10 packets.

Below is a zoomed in view of a 10 minute timespan that better shows Mode 1 and 2.

[.text-center]
.Round Trip Time for 200 byte payload (10min)
image::udp/rtt-baseline-zoomed.png[]

So far I have not been successful in determining the root cause of these modes. All 3 modes seem to be related to actual time and independent of rate and packet count. However, after several tests, I strongly suspect that all of them occur on the firmware side rather than being tied to the switch or the protocol itself. With more tuning and a better implementation (e.g. lwip with zero copy and tuned options) it seems likely that the maximum jitter could go down to below 6us and potentially even 1us.

// test IO board to IO board (100)

//operating system jitter, network jitter, clock drift (reference IEEE 1588v2)

// The sporadic outliers at ~210us happen exactly every 5s according to system clock. If it were a cron job in the switch, the clock would eventually drift apart. Note that it may also be every 500 packets because there is almost zero jitter.

// The small outlier bursts happen on average every 322.5s and last for on average 15.0105s

== Latency cost per Switch (100/1000)

As mentioned in the intro, all switches I currently have access to use a 'store-and-forward' approach that requires the switch to fully receive a packet before forwarding it appropriately. Therefore, the latency cost per switch is the time it takes a packet on the wire plus any switching overhead. The wire time is constant (2us or 20us for 266 bytes), but the overhead depends on the switch implementation. There is not much 3rd party data out there, so depending on your requirements you may need to conduct your own benchmarks if you need to evaluate hardware.

For this benchmark I tested three switches that were individually added to the baseline setup: 

* http://ww1.microchip.com/downloads/en/DeviceDoc/KSZ8863MLL_FLL_RLL_DS.pdf[MICREL KSZ8863] (100 Mbit/s) embedded in an X5 actuator
* http://www.downloads.netgear.com/files/GDC/GS105/GS105_datasheet_04Sept03.pdf[NETGEAR ProSAFE GS105] (1 Gbit/s)
* https://routerboard.com/RB750Gr2[MikroTik RB750Gr2 (RouterBOARD hEX)] (1 Gbit/s) with DHCP disabled

.
|====  
| image:udp/io-boards-100mbit-switch.jpg[] | image:udp/io-boards-gbit-switch.jpg[]  
|====

image::udp/rtt-switch-comparison.png[]

image::udp/comparison-switch-latency.png[]

Since the Netgear GS105 exhibited bad performance in this test, I ran another baseline test using the RB750Gr2 instead of the GS105. While there was a consistent gain of 0.5us, I did not consider this significant enough to rerun all tests.

== Larger networks

40 IO boards responding within <1 us.

image::udp/multiple-boards.jpg[]

http://www.downloads.netgear.com/files/GDC/GS748Tv1/GS748T_ds_03Feb05.pdf[NETGEAR ProSAFE GS748T]

image::udp/os-jitter-linux-rt.png[]

image::udp/rtt-40x.png[]

More than 37 million packets. No packet drops or re-ordering of a source stream. Also tried 1KHz and result looks the same (but too much data to plot at once).

image::udp/rtt-40x-zoomed.png[]

== Conclusion

