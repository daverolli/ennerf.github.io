= A Practical Look at Latency in Robotics : Ethernet and UDP
:published_at: 2016-11-05
:hp-tags: Latency, Ethernet, UDP
:imagesdir: ../images

== Intro

The User Datagram Protocol (UDP) is unreliable, packets may be lost, reordered, and could even be delayed by an arbitrarily long time. These shortcomings make standard UDP seem non-deterministic and a really bad choice for controlling real-time systems. Luckily modern network components have largely solved these issues and performance in practice can be surprisingly good.

Early networks were built using 'Hubs' that share a single channel for all network ports. This results in a large https://en.wikipedia.org/wiki/Collision_domain[Collision Domain] that can cause packet loss whenever more than one sender transmits a packet at the same time. Nowadays Hubs have been replaced with 'Switches' that have dedicated connections for each port in order to isolate collision domains. Connections also tend to be full-duplex, which means that there are dedicated lines for transmitting and receiving data. Since all communications are done over dedicated lines, collisions on the physical (wire) level should be a thing of the past.

Virtually all consumer switches today use the https://en.wikipedia.org/wiki/Store_and_forward[Store-and-Forward] switching architecture. They fully receive the packet, store them in an internal buffer, and then forward them to the appropriate receiver port. This adds a latency cost that scales linearly with the number of switches that a packet has to go through. There is an alternative approach called https://en.wikipedia.org/wiki/Cut-through_switching[Cut-through] switching that can forward packets while they are being received. While this can theoretically lower latency, it is not very popular because it's more difficult to implement and only works well if both ports negotiate the same speed and the receiver port isn't in use. There is also remaining requirement to buffer enough of a packet to evaluate the target address in the header, so the benefits primarily effect larger packets.

// Ethercat has solved this in a somewhat elegant way by not using device addresses and by limiting each wire to a single writer. 

// Depending on the use case this can have positive impacts on latency. However, it still requires enough buffering to evaluate the header address and only works well if both sender and receiver ports negotiate the same speed and the sender port isn't already being used.

// In the common https://en.wikipedia.org/wiki/Store_and_forward[Store-and-Forward] switching architecture all packets get buffered by the Switch and are then forwarded to the appropriate receiver port. This adds a latency cost that scales linearly with the number of switches that a packet has to go through. The alternative https://en.wikipedia.org/wiki/Cut-through_switching[Cut-through] switching approach can forward packets before they are fully received, which can have positive impacts on latency especially with very large packets.

[NOTE]
TODO: improve the flow of this section

//When more than one component try to communicate, there may be a collision that causes packets to be dropped. Modern networks use 'Switches' that have dedicated connections for each port to isolate collision domains. Additionally, connections tend to be full-duplex, so there are dedicated lines for transmitting and receiving data. Depending on the architecture, packets get buffered and sent to the appropriate port (Store-and-Forward) or directly passed through without evaluation (Cut-Through). Thus, switched networks should never have any packets that get dropped due to collisions. Note that packets can still be lost due to buffer overflows and/or too much load on a system. There is also a latency cost associated with each 'hop' (or buffer) that a packet has to go through.

// re-read
// http://www.cisco.com/c/en/us/products/collateral/switches/nexus-5020-switch/white_paper_c11-465436.html
// https://www.lantronix.com/resources/networking-tutorials/network-switching-tutorial/
// http://www.embedded.com/design/connectivity/4023291/Real-Time-Ethernet

Similarly, reordering (in the sense that a stream of packets coming from a single source can arrive out of order) may be relevant for internet communications, but generally does not apply to local networks without redundant routes. Depending on the driver implementation it can theoretically happen on a local network, but I have yet to observe a single case.

There are a number of competing networking standards that originated in industrial automation that are built on top of standard Ethernet. They are able to guarantee certain levels of determinism by enforcing tight control over the network layout and by limiting the components that can be connected. However, controlling standard networks in a similar manner can also produce quite good results.

== Benchmark Setup

A common way to benchmark networks is to setup two computers and have a sender transmit a message to a receiver that echoes it back. That way the sender can measure the round trip time and gather statistics of the network. This works well, but there is a lot of 'magic' that happens in the underlying operating system and it's device drivers.

In order to mitigate this potential source of jitter, I setup a benchmark that measures the round trip time between two dedicated embedded systems. The overall approach remains the same, i.e.,

1. Sender sends packet to receiver in fixed intervals
2. Receiver echoes packet back to sender
3. Sender measures the round trip time
4. Measurement gets persisted to disk

As the sender and receiver devices I'm using two HEBI Robotics I/O Boards. It's a product that we haven't advertised yet, but customers usually use it to integrate actuators with external devices or to get various sensor input into MATLAB. It has 48 pins that serve a variety of functions (analog/digitial I/O, PWM, Encoder input, etc.) that can be accessed remotely via network. It sports a 168MHz ARM microcontroller (STM32f407) and a 100 Mbit network port. In order to make sure that the devices are not doing anything else, I programmed custom firmware specifically for testing the network stack. The relevant code pieces can be found https://gist.github.com/ennerf/36a57d432bcff20a58efcdee10f91bd9[here]. The hardware timer (counter) resolution was set to 250ns.

image::udp/io-boards.jpg[HEBI Robotics IO Board]

Since storing multiple Gigabytes of data on an embedded device is challenging, I setup a https://gist.github.com/ennerf/0ddc4396d15852d28e4eca4a8a923eb7[UDP server (Java)] that receives measurement data and persists the data to disk. The main socket handler writes incoming packets into a double buffered structure that can be persisted by a background thread without halting the packet handler. The synchronization between the threads is done using a http://stuff-gil-says.blogspot.com/2014/11/writerreaderphaser-story-about-new.html[WriterReaderPhaser], which is a synchronization primitive that is very useful for persisting events that are represented by a small amount of data.

// Alternatively: oscilloscope and logic analyzer

The resulting binary data was loaded into MATLAB(C) for analysis and to generate the plots. The code can be found https://gist.github.com/ennerf/19b48406a066f6e946a0567a1a4de1ed[here].

== Ethernet Packet Structure

UDP datagrams include various types of overhead in addition to the actual payload. The minimum overhead is 66 bytes. Additionally, Ethernet Frames have a minimum size of 84 bytes, which makes the minimum payload for a UDP datagram 18 bytes.

.UDP/IP Packet Size
[width="100%",options="header,footer",cols="a,a,a"]
|====================
| Protocol | Item | Size (byte) 

| https://en.wikipedia.org/wiki/Ethernet_frame[Ethernet II] | Preamble | 7
|  | Start of Frame Delimiter | 1
|  | MAC Header + CRC checksum | 18 (no optional fields)
|  | Interpacket gap | 12

| https://en.wikipedia.org/wiki/IPv4[Internet Protocol (IPv4)] | IP Header | 20  (no optional fields)

| https://en.wikipedia.org/wiki/User_Datagram_Protocol[User Datagram Protocol (UDP)] | UDP Header | 8

|====================

All of the benchmarks in this post measure the round-trip time for a UDP datagram with a payload of 200 bytes. After including all overhead, the actual frame size on the wire is 266 bytes. For comparison, the feedback packets of HEBI Robotic's X-Series actuators currently require a payload of 185-215 bytes, and requesting the state of all pins of an I/O board currently requires about 300 bytes.

While the size is representative of a typical feedback packet, the round trip times in practice are slightly faster because the outgoing packets (commands) are usually significantly smaller than response packets (feedback).

The theoretical time to send 266 bytes over the wire is 20.3us over 100 Mbit/s Ethernet and 2.03us over 1 Gbit/s Ethernet.

== Baseline: Single Switch

We can establish a baseline of the best-case round trip time (RTT) by having both devices communicate with each other through a single switch that does not see any external traffic. It's possible to create a point-to-point connection without any switches, but the typical use case goes through at least one switch.

I set the frequency to 100Hz and logged data for close to 24 hours. I chose this frequency because it is a common control rate for sending high-level trajectories, and because 10ms is a safe deadline in case there are large outliers. Faster control rates are of course possible.

First, let's look at the jitter of the underlying embedded real-time operating system. The figure below shows the difference between an idealized signal that ticks every 10ms and the actual measurements of the loop start times. 99% are within the lowest measurement resolution (250ns), and the worst observed case is below 6us. This is quite a bit better than the 150us worst observed case on real-time Linux as setup in  https://ennerf.github.io/2016/09/20/A-Practical-Look-at-Latency-in-Robotics-The-Importance-of-Metrics-and-Operating-Systems.html[The Importance of Metrics and Operating Systems].

[.text-center]
.OS jitter of ChibiOS 2.6.8 on STM32F407 (24h)
image::udp/os-jitter-embedded.png[link="/images/udp/os-jitter-embedded.png"]

The figure below shows the round trip time for all packets and the corresponding percentile distribution. There were a total of 8.5 million messages. None of them were lost and none of them arrived out of order.

[[img-rtt-24h]]
[.text-center]
.Round Trip Time for 200 byte payload (24h)
image::udp/rtt-baseline.png[]

90% of all packets arrive within 194us and a jitter of less than 1 microsecond. Roughly 80us of this time is spent on the wire, so using chips that support Gigabit (rather than 100Mbit) could lower the round trip time to ~120us. Above the common case, there are three different periodically reoccuring modes that cause the round-trip-time to get up to a worst case that is 60us higher. 

* Mode 1 occurs consistently every ~5.3 minutes and lasts for ~15.01 seconds. During this time it adds up to 4 us latency.
* Mode 2 occurs exactly once every 5 seconds and is always at 210us.
* Mode 3 occurs roughly once an hour and adds linearly increasing latency up to 60us to 10 packets.

So far I have not been successful in determining the root cause of these modes. All three modes seem to be related to actual time and independent of rate and packet count. However, after several tests, I strongly suspect that all of them occur on the firmware side rather than being tied to the switch or the protocol itself. Below is a zoomed in view of a 10 minute time span that better shows Modes 1 and 2.

[.text-center]
.Round Trip Time for 200 byte payload (10min)
image::udp/rtt-baseline-zoomed.png[]

Overall this initial data looks very promising for being able to use UDP for many real-time control tasks. With more tuning and a better implementation (e.g. lwip with zero copy and tuned options) it seems likely that the maximum jitter could go down to below 6us and potentially even 1us.

// test IO board to IO board (100)

//operating system jitter, network jitter, clock drift (reference IEEE 1588v2)

// The sporadic outliers at ~210us happen exactly every 5s according to system clock. If it were a cron job in the switch, the clock would eventually drift apart. Note that it may also be every 500 packets because there is almost zero jitter.

// The small outlier bursts happen on average every 322.5s and last for on average 15.0105s

== Switching Cost

As mentioned in the intro, most modern switches use the 'store-and-forward' approach that requires the switch to fully receive a packet before forwarding it appropriately. Therefore, the latency cost per switch is the time it takes a packet on the wire plus any switching overhead. The wire time is constant (2.03us or 20.3us for 266 bytes), but the overhead depends on the switch implementation. There is not much 3rd party data out there, so depending on your requirements you may need to conduct your own benchmarks if you need to evaluate hardware.

For this benchmark I tested three switches that were individually added to the baseline setup.

[frame="none"]
|====
| image:udp/io-boards-100mbit-switch.jpg[] | image:udp/io-boards-gbit-switch.jpg[]
|====

This benchmark received a combined total of about 1 million packets. None of them were dropped or received out of order.

image::udp/rtt-switch-comparison.png[]

Both KSZ8863 and the RB750Gr2 add a constant switching latency of 2.9 us and 3.6 us in addition to the wire time of 40.6 us and 4.06 us respectively to the RTT. The added jitter seems to be negligible at well below 1us.

[width="100%",options="header",cols="3a,1a,1a,1a"]
|====
| Switch | Connection | 90%-ile RTT | Overhead

| Baseline | - | 193.8 us | -

| http://ww1.microchip.com/downloads/en/DeviceDoc/KSZ8863MLL_FLL_RLL_DS.pdf[MICREL KSZ8863] (embedded in X5 actuator)
| 100 Mbit/s | +43.5 us | 2.9 us

| http://www.downloads.netgear.com/files/GDC/GS105/GS105_datasheet_04Sept03.pdf[NETGEAR ProSAFE GS105]
| 1 Gbit/s | +51.0 us | 47 us

| https://routerboard.com/RB750Gr2[MikroTik RB750Gr2 (RouterBOARD hEX)]
| 1 Gbit/s | +7.7 us | 3.6 us

|====

// 90% percentiles
// KSZ8863: 237.3 us
// GS105: 244.8 us
// RB750Gr2: 201.5 us

[NOTE]
The RB750Gr2 is technically a router, but after disabling DHCP and not using the WAN port, it effectively acts like a switch.

Surprisingly, the GS105 seems to have problems with this particular use case, resulting in higher latency and more jitter than the KSZ8863 even though it was connected using Gigabit. According to the spec sheet, the added network latency should be below 10us (1 Gbit) and 20us (100 Mbit). I did additional tests and it did seem to perform according to spec when using exclusively 100 Mbit/s or 1 Gbit/s on all ports.

Mode 2 (at 210 us) seems to disappear for higher round trip times, indicating an issue at the receiving step of the sender. Modes 1 and 3 do not seem to be affected by additional switches, indicating that they are caused by something happening on on the firmware side. The figure below shows a zoomed view of the time series highlighting the added jitter characteristics.

image::udp/comparison-switch-latency.png[]

Since the Netgear GS105 exhibited bad performance in this test, I ran another baseline test using the RB750Gr2 instead of the GS105. While there was a consistent gain of 0.5us, I did not consider this significant enough to rerun all tests.

== Larger Networks

Whenever I get this far in a conversation, I usually get push back saying that this would of course be the case for single devices, but that UDP will blow up once more than one thing is connected.

In order to test this hypothesis we put together a test setup consisting of 40 HEBI Robotics I/O boards that are connected to a  http://www.downloads.netgear.com/files/GDC/GS748Tv1/GS748T_ds_03Feb05.pdf[GS748T] 48-port Ethernet switch. Each board runs the 'receiver' firmware. This enables us to send a single broadcast message that gets received by all devices at approximately the same time. This then triggers 40x266 byte (>10KB total) return packets that can arrive at the switch within less than 250 nanoseconds. This puts quite a bit of pressure on the switch and would be representative of a very high degree of freedom system such as a full body humanoid robot.

image::udp/multiple-boards.jpg[]

In order to keep the sender device from overloading, I changed the sender to a Desktop computer running https://ennerf.github.io/2016/09/20/A-Practical-Look-at-Latency-in-Robotics-The-Importance-of-Metrics-and-Operating-Systems.html[Scientific Linux 6.6 with real-time kernel]. First, I looked at the jitter of the underlying operating system. The figure below shows the difference between an idealized signal that ticks every 10ms and the actual measurements of the loop start times. It shows that this setup suffers from more than an order of magnitude more jitter than the embedded OS. (Note that the corresponding jHiccup control chart looks identical as in the OS blog post.)

image::udp/os-jitter-linux-rt.png[]

Gathering data at 100Hz close to 90 minutes resulted in more than 20 million returned packets. I also ran some tests at 1KHz and the result looks the same.

image::udp/rtt-linux-40x-zoomed.png[]

It may be surprising, but there was again no packet loss or re-ordering of packets from a single source. Rather than blowing up, what actually happens is that all packets get stored in the internal buffer of the switch (1.6MB), queued, and forwarded to the target port as fast as possible. Since the original sender is connected via 1 Gigabit, the packets arrive every ~2us. All of them have the same start timestamp, so they show up as a vertical column in the graphs.

image::udp/rtt-linux-40x.png[]

What surprised me is that the latency is so much higher than the previous tests. Looking at the previous tests, I would have expected most columns to start at around ~180us and end at ~280us. While this is sometimes the case, the majority of columns start above 300 us. After some initial research I suspect that this is related to the Linux https://en.wikipedia.org/wiki/New_API[NAPI] using polling mode rather than interrupts.

// Single unicast on LinuxRT has almost the same performance (minux 50us at the top). The 4 low bars may be polling intervals after the kernel driver switches to polling mode. Will take additional baseline-like data for Linux. From what I've read, sending should be non-blocking as long as the send buffer isn't full and the OS doesn't after the sys call.
// See:
// see https://lwn.net/Articles/551284/
// https://en.wikipedia.org/wiki/New_API
// http://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/

== Conclusion

Even consumer-grade Ethernet networks are actually very deterministic. Re-ordering is not an issue on local networks and unless ports get saturated, there should be no packet loss. Almost all of the commonly observed jitter seems to be caused by the operating system and it's device drivers. The chart below compares the Linux system communicating (using unicast) with a single device in the same setup as the baseline test.

image::udp/rtt-linux-1x.png[]

[NOTE]
TODO: fix the plot and extend text. Maybe add another plot testing 4.x kernel in Ubuntu?

I still wouldn't recommend using consumer networking equipment for controlling extremely dangerous industrial robots, but for most robotic applications standard UDP is going to be more than sufficient.

I realize that this post still leaves many questions, but it's already a bit too long as it is. I may do another post in the future that looks at additional cases such as differences in operating systems, network cards, and performance degradation when sending uncontrolled traffic (e.g. streaming video) through the same network. There may also be a blog post discussing clock drift and clock synchronization using IEEE 1588v2.


