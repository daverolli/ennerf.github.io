= A Practical Look at Latency in Robotics : Ethernet and UDP
:published_at: 2016-11-05
:hp-tags: Latency, Ethernet, UDP
:imagesdir: ../images

== Introduction

Popular wisdom claims that Ethernet is non-deterministic and that UDP can't be used for real-time control. This post looks at the data to check whether popular wisdom is wrong.

The User Datagram Protocol (UDP) is unreliable, packets may be lost, reordered, and could even be delayed by an arbitrarily long time. These shortcomings make standard UDP seem non-deterministic and a really bad choice for controlling real-time systems. Luckily modern network components have largely solved these issues and performance in practice can be surprisingly good.

== Background

Much of this popular wisdom comes from the early days of networking. Devices used to communicate over a single shared media that used https://en.wikipedia.org/wiki/Carrier_sense_multiple_access_with_collision_detection[CSMA/CD]
(carrier sense multiple access with collision detection) for arbitrating access. After detecting a collision, there were potentially multiple attempts to retransmit the packet after random timeouts, and maybe drop it. By connecting more devices through https://en.wikipedia.org/wiki/Ethernet_hub[Hubs] the https://en.wikipedia.org/wiki/Collision_domain[Collision Domain] was extended further, resulting in more collisions and unpredictable behavior.

In a process that started in https://en.wikipedia.org/wiki/Kalpana_(company)[1990], Hubs have been replaced with https://en.wikipedia.org/wiki/Network_switch[Switches] that have dedicated full-duplex (separate lines for transmitting and receiving data) connections for each port in order to separate segments and isolate collision domains. This gets rid of any collisions that were happening on the physical (wire) level. CSMA/CD is still supported for backwards compatibility and half-duplex connections, but it is largely obsolete.

In the https://en.wikipedia.org/wiki/Store_and_forward[Store-and-Forward] switching architecture that is implemented by virtually all switches, switches fully receive packets, store them in an internal buffer, and then forward them to the appropriate receiver port. This adds a latency cost that scales linearly with the number of switches that a packet has to go through.

In the alternative https://en.wikipedia.org/wiki/Cut-through_switching[Cut-through] approach switches can forward packets immediately once the address has been received, potentially resulting in lower latency. While this is sometimes used in latency sensitive applications, such as financial trading applications, it has never become popular in consumer grade hardware. It's more difficult to implement, only works well if both ports negotiate the same speed, and if the receiver port isn't already in use. There is also still the requirement to buffer enough data to evaluate the target address, so the benefits primarily effect larger packets.

// Ethercat has solved this in a somewhat elegant way by not using device addresses and by limiting each wire to a single writer. 

// re-read
// http://www.cisco.com/c/en/us/products/collateral/switches/nexus-5020-switch/white_paper_c11-465436.html
// https://www.lantronix.com/resources/networking-tutorials/network-switching-tutorial/
// http://www.embedded.com/design/connectivity/4023291/Real-Time-Ethernet

Another often cited problem is https://en.wikipedia.org/wiki/Out-of-order_delivery[Out-of-Order Delivery], which says that a sequence of packets coming from a single source may be received in a different order. This is relevant for communicating over the internet, but generally does not apply to local networks without redundant routes and load balancing. Depending on the driver implementation it can theoretically happen on a local network, but I have yet to observe such a case.

There are several competing networking standards that are built on Ethernet and can guarantee enough determinism to be used in industrial automation (https://en.wikipedia.org/wiki/Industrial_Ethernet[Industrial Ethernet]). They achieve this by enforcing tight control over the network layout and by limiting the components that can be connected. However, even cheap consumer grade network equipment can produce very good results if the network is controlled in a similar manner.

== Benchmark Setup

A common way to benchmark networks is to setup two computers and have a sender transmit a message to a receiver that echoes it back. That way the sender can measure the round trip time and gather statistics of the network. This works well, but there is a lot of 'magic' that happens in the underlying operating system and it's device drivers.

In order to mitigate this potential source of jitter, I setup a benchmark that measures the round trip time between two dedicated embedded systems. The overall approach remains the same, i.e.,

1. Sender sends packet to receiver in fixed intervals
2. Receiver echoes packet back to sender
3. Sender measures the round trip time
4. Measurement gets persisted to disk

As the sender and receiver devices I'm using two HEBI Robotics I/O Boards. It's a product that we typically don't advertise, but customers often use it to integrate actuators with external devices or to get various sensor input into MATLAB. It has 48 pins that serve a variety of functions (analog/digitial I/O, PWM, Encoder input, etc.) that can be accessed remotely via network. It contains a 168MHz ARM microcontroller (STM32f407) and a 100 Mbit network port.

[.text-center]
.HEBI Robotics IO Board
image::udp/io-boards.jpg[link="../images/udp/io-boards.jpg"]

[NOTE]
TODO: Replace with 'whitewashed' product picture?

I wrote and programmed custom firmware based on http://www.chibios.org/[ChibiOS 2.6.8] and http://savannah.nongnu.org/projects/lwip/[lwIP 1.4.1] to isolate the network stack. The relevant code pieces can be found https://gist.github.com/ennerf/36a57d432bcff20a58efcdee10f91bd9[here]. The elapsed time is measured using a hardware counter that was set to a 250ns resolution.

Since storing multiple Gigabytes of data on an embedded device is challenging, I setup a https://gist.github.com/ennerf/0ddc4396d15852d28e4eca4a8a923eb7[UDP server (Java)] that receives measurement data and persists the data to disk. The main socket handler writes incoming packets into a double buffered structure that can be persisted by a background thread without halting the packet handler. The synchronization between the threads is done using a http://stuff-gil-says.blogspot.com/2014/11/writerreaderphaser-story-about-new.html[WriterReaderPhaser], which is a synchronization primitive that is very useful for persisting events that are represented by a small amount of data.

// Alternatively: oscilloscope and logic analyzer

The resulting binary data was loaded into MATLAB(C) for analysis and to generate the plots. The code can be found https://gist.github.com/ennerf/19b48406a066f6e946a0567a1a4de1ed[here].

== Ethernet Packet Structure

UDP datagrams include various types of overhead in addition to the actual payload. The minimum overhead is 66 bytes. Additionally, Ethernet Frames have a minimum size of 84 bytes, which makes the minimum payload for a UDP Datagram 18 bytes.

.UDP Datagram Size
[width="100%",options="header,footer",cols="a,a,a"]
|====================
| Protocol | Item | Size (byte) 

| https://en.wikipedia.org/wiki/Ethernet_frame[Ethernet II] | Preamble | 7
|  | Start of Frame Delimiter | 1
|  | MAC Header + CRC checksum | 18 (no optional fields)
|  | Interpacket gap | 12

| https://en.wikipedia.org/wiki/IPv4[Internet Protocol (IPv4)] | IP Header | 20  (no optional fields)

| https://en.wikipedia.org/wiki/User_Datagram_Protocol[User Datagram Protocol (UDP)] | UDP Header | 8

|====================

For these benchmarks I chose to measure the round-trip time for UDP Datagrams with a payload of 200 bytes. For comparison, the feedback packets of HEBI Robotic's X-Series actuators currently require 185-215 bytes. Returning the state of all pins of an I/O board currently requires about 300 bytes.

While the size is representative of a typical feedback packet, the round trip times in production will be slightly faster because the outgoing packets (commands) are usually significantly smaller than response packets (feedback).

After including all overhead, the actual Ethernet frame size on the wire is 266 bytes. The theoretical time it takes to transfer 266 bytes over 100 Mbit/s and 1Gbit/s Ethernet is 20.3us and 2.03us respectively.

== Baseline: Single Switch

We can establish a baseline of the best-case round trip time (RTT) by having both devices communicate with each other through a single switch that does not see any external traffic. It's possible to create a point-to-point connection without any switches, but the typical use case goes through at least one switch.

[.text-center]
.Baseline setup using a single switch
image::udp/io-boards2.jpg[link="../images/udp/io-boards2.jpg"]

I set the frequency to 100Hz and logged data for close to 24 hours. I chose this frequency because it is a common control rate for sending high-level trajectories, and because 10ms is a safe deadline in case there are large outliers. Faster control rates are of course possible.

First, let's look at the jitter of the underlying embedded real-time operating system. The figure below shows the difference between an idealized signal that ticks every 10ms and the actual measurements of the loop start times. 99% are within the lowest measurement resolution (250ns), and the worst observed case is below 6us. This is quite a bit better than the 150us worst observed case on real-time Linux as setup in  https://ennerf.github.io/2016/09/20/A-Practical-Look-at-Latency-in-Robotics-The-Importance-of-Metrics-and-Operating-Systems.html[The Importance of Metrics and Operating Systems].

[.text-center]
.OS jitter of ChibiOS 2.6.8 on STM32F407 (24h)
image::udp/os-jitter-embedded.png[link="../images/udp/os-jitter-embedded.png"]

The figure below shows the round trip time for all packets and the corresponding percentile distribution. There were a total of 8.5 million messages. None of them were lost and none of them arrived out of order.

[[img-rtt-24h]]
[.text-center]
.Round Trip Time for 200 byte payload (24h)
image::udp/rtt-baseline.png[link="../images/udp/rtt-baseline.png"]

90% of all packets arrive within 194us and a jitter of less than 1 microsecond. Roughly 80us of this time is spent on the wire, so using chips that support Gigabit (rather than 100Mbit) could lower the round trip time to ~120us. Above the common case, there are three different periodically reoccuring modes that cause the round-trip-time to get up to a worst case that is 60us higher. 

* Mode 1 occurs consistently every ~5.3 minutes and lasts for ~15.01 seconds. During this time it adds up to 4 us latency.
* Mode 2 occurs exactly once every 5 seconds and is always at 210us.
* Mode 3 occurs roughly once an hour and adds linearly increasing latency up to 60us to 10 packets.

So far I have not been successful in determining the root cause of these modes. All three modes seem to be related to actual time and independent of rate and packet count. However, after several tests, I strongly suspect that all of them occur on the firmware side rather than being tied to the switch or the protocol itself. Below is a zoomed in view of a 10 minute time span that better shows Modes 1 and 2.

[.text-center]
.Round Trip Time for 200 byte payload (10min)
image::udp/rtt-baseline-zoomed.png[link="../images/udp/rtt-baseline-zoomed.png"]

Overall this initial data looks very promising for being able to use UDP for many real-time control tasks. With more tuning and a better implementation (e.g. lwip with zero copy and tuned options) it seems likely that the maximum jitter could go down to below 6us and potentially even 1us.

// test IO board to IO board (100)

//operating system jitter, network jitter, clock drift (reference IEEE 1588v2)

// The sporadic outliers at ~210us happen exactly every 5s according to system clock. If it were a cron job in the switch, the clock would eventually drift apart. Note that it may also be every 500 packets because there is almost zero jitter.

// The small outlier bursts happen on average every 322.5s and last for on average 15.0105s

== Switching Cost

As mentioned in the intro, most modern switches use the 'store-and-forward' approach that requires the switch to fully receive a packet before forwarding it appropriately. Therefore, the latency cost per switch is the time it takes a packet on the wire plus any switching overhead. The wire time is constant (2.03us or 20.3us for 266 bytes), but the overhead depends on the switch implementation. There is not much 3rd party data out there, so depending on your requirements you may need to conduct your own benchmarks if you need to evaluate hardware.

For this benchmark I tested three switches that were individually added to the baseline setup.

[.text-center]
.Switches added to the baseline setup
[frame="none"]
|====
| image:udp/io-boards-100mbit-switch.jpg[link="../images/udp/io-boards-100mbit-switch.jpg"] | image:udp/io-boards-gbit-switch.jpg[link="../images/udp/io-boards-gbit-switch.jpg"]
|====

This benchmark received a combined total of about 1 million packets. None of them were dropped or received out of order.

[.text-center]
.Comparison of round trip time through different switches (35min)
image::udp/rtt-switch-comparison.png[link="../images/udp/rtt-switch-comparison.png"]

Mode 2 (at 210 us) seems to disappear for higher round trip times, indicating an issue at the receiving step of the sender. Modes 1 and 3 do not seem to be affected by additional switches, indicating that they are caused by something happening on on the firmware side. The figure below shows a zoomed view of the time series highlighting the added jitter characteristics.

[.text-center]
.Zoomed in view of switch comparison (10min)
image::udp/comparison-switch-latency.png[link="../images/udp/comparison-switch-latency.png"]

Both KSZ8863 and the RB750Gr2 add a constant switching latency of 2.9 us and 3.6 us in addition to the wire time of 40.6 us and 4.06 us respectively to the RTT. The added jitter seems to be negligible at well below 1us.

[width="100%",options="header",cols="3a,1a,1a,1a"]
|====
| Switch | Connection | 90%-ile RTT | Overhead (not-on-wire)

| Baseline | 2x 100 MBit/s | 193.8 us | 112.6 us

| http://ww1.microchip.com/downloads/en/DeviceDoc/KSZ8863MLL_FLL_RLL_DS.pdf[MICREL KSZ8863] (embedded in X5 actuator)
| 100 Mbit/s | +43.5 us | 2.9 us

| http://www.downloads.netgear.com/files/GDC/GS105/GS105_datasheet_04Sept03.pdf[NETGEAR ProSAFE GS105]
| 1 Gbit/s | +51.0 us | 47 us

| https://routerboard.com/RB750Gr2[MikroTik RB750Gr2 (RouterBOARD hEX)]
| 1 Gbit/s | +7.7 us | 3.6 us

|====

// 90% percentiles
// KSZ8863: 237.3 us
// GS105: 244.8 us
// RB750Gr2: 201.5 us

[NOTE]
The RB750Gr2 is technically a router, but after disabling DHCP and not using the WAN port, it effectively acts like a switch.

Surprisingly, the GS105 seems to have problems with this particular use case, resulting in higher latency and more jitter than the KSZ8863 even though it was connected using Gigabit. According to the spec sheet, the added network latency should be below 10us (1 Gbit) and 20us (100 Mbit). I did additional tests and it did seem to perform according to spec when using exclusively 100 Mbit/s or 1 Gbit/s on all ports.

I also conducted another baseline test using the RB750Gr2 instead of the GS105 to see whether this issue effects the baseline case as well. While there was a consistent improvement of 0.5us, I did not consider this significant enough to rerun all tests.

== Micro-bursting

When I get this far in a conversation, I often get push back claiming that while this may be true for single devices, UDP will surely blow up once more than one thing is connected.

In order to test the actual behavior we put together a test setup consisting of 40 HEBI Robotics I/O boards that are connected to a  http://www.downloads.netgear.com/files/GDC/GS748Tv1/GS748T_ds_03Feb05.pdf[GS748T] 48-port Ethernet switch. Each board runs the 'receiver' firmware. Sending a single broadcast message to all receiver devices triggers 40x266 byte (>10KB total) return packets that can arrive at the switch within less than 250 nanoseconds. This https://en.wikipedia.org/wiki/Micro-bursting_(networking)[Microburst] traffic pattern is representative of a very high degree of freedom system such as a full body humanoid robot.

[.text-center]
.Network test setup with 40 HEBI Robotics IO Boards
image::udp/multiple-boards.jpg[link="../images/udp/multiple-boards.jpg"]

[NOTE]
TODO: Replace with a new picture that doesn't have a heater in the back?

This setup also benefits from two convenient side effects of using standard networking:

1. There is no need for any manual addressing because of https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol[DHCP] and device specific globally unique mac addresses.

2. It takes only 3-6 seconds to re-program the firmware of all 40 devices at once using a bootloader and TCP/IP.

In order to keep the sender device from overloading, I changed the sender to a http://www.gigabyte.com/products/product-page.aspx?pid=4888#ov[Gigabyte Brix i7-4770R] computer running https://ennerf.github.io/2016/09/20/A-Practical-Look-at-Latency-in-Robotics-The-Importance-of-Metrics-and-Operating-Systems.html[Scientific Linux 6.6 with real-time kernel] with a disabled firewall. First, I looked at the jitter of the underlying operating system. The figure below shows the difference between an idealized signal that ticks every 10ms and the actual measurements of the loop start times. It shows that this setup suffers from more than an order of magnitude more jitter than the embedded OS. (Note that the corresponding jHiccup control chart looks identical as in the OS blog post.)

[.text-center]
.Operating system jitter of Scientific Linux 6.6 with MRG Realtime
image::udp/os-jitter-linux-rt.png[link="../images/udp/os-jitter-linux-rt.png"]

Gathering data at 100Hz close to 90 minutes resulted in more than 20 million returned packets. I also ran some tests at 1KHz and the result looks the same.

[.text-center]
.Receive pattern for incoming datagrams from 40 devices
image::udp/rtt-linux-40x-zoomed.png[link="../images/udp/rtt-linux-40x-zoomed.png"]

It may be surprising, but there was again no packet loss or re-ordering of packets from a single source. Rather than blowing up, what actually happens is that all packets get stored in the internal buffer of the switch (1.6MB), queued, and forwarded to the target port as fast as possible. Since the original sender is connected via 1 Gigabit, the packets arrive every ~2us. All of them have the same start timestamp, so they show up as a vertical column in the graphs.

[.text-center]
.Round-trip latency for datagrams from 40 devices (90 min)
image::udp/rtt-linux-40x.png[link="../images/udp/rtt-linux-40x.png"]

However, what did actually surprise me was how much worse the latency turned out to be compared to the embedded system. I expected most columns to start at around ~180us and end at ~280us. While this is sometimes the case, the majority of columns start above 300 us. After some initial research I suspect that this is related to suboptimal device drivers, and the Linux https://en.wikipedia.org/wiki/New_API[NAPI] using polling mode rather than interrupts. I remember reading about some of this in the past, but I didn't expect this order of magnitude. The installed network interface and driver are below.

[source,shell]
$ lspci | grep Ethernet

03:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller (rev 0c)

[source,shell]
$ sudo dmesg | grep "Ethernet driver"

r8169 Gigabit Ethernet driver 2.3LK-NAPI loaded

// Single unicast on LinuxRT has almost the same performance (minux 50us at the top). The 4 low bars may be polling intervals after the kernel driver switches to polling mode. Will take additional baseline-like data for Linux. From what I've read, sending should be non-blocking as long as the send buffer isn't full and the OS doesn't after the sys call.
// See:
// see https://lwn.net/Articles/551284/
// https://en.wikipedia.org/wiki/New_API
// http://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/

== Conclusion

It seems that even consumer-grade Ethernet networks actually exhibit very deterministic performance with regards to latency. There were also no lost packets and none were received out-of-order. Large and extremely dangerous industrial robots should still only be controlled using specialized networking equipment, but for most robotic applications standard UDP is going to be more than sufficient.

The unexpectedly high latency and jitter that is commonly observed in network benchmarks seem to be caused almost entirely by the underlying operating system and it's device drivers. The two charts below show a comparable baseline setup with one sender running on an embedded device and one running on a RT Linux system over 10 hours and 10 minutes respectively. Note that the RT Linux system is connected using a Gigabit connection and should actually receive datagrams ~40us before the embedded device does.

[.text-center]
.Baseline setup on embedded device vs Linux computer (10h)
image::udp/rtt-linux-1x-comparison-10h.png[link="../images/udp/rtt-linux-1x-comparison-10h.png"]

[.text-center]
.Baseline setup on embedded device vs Linux computer (10min)
image::udp/rtt-linux-1x-comparison-10m.png[link="../images/udp/rtt-linux-1x-comparison-10m.png"]

I realize that this post still leaves many open questions, but it's already quite long as it is. I'm considering doing another post in the future that looks at additional cases such as differences in operating systems, network interfaces, device drivers, and the performance impacts of sending uncontrolled traffic (e.g. streaming video) through the same network. There may also be a blog post discussing clock drift and clock synchronization using IEEE 1588v2.


