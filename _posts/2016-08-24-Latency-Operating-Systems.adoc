= Latency : Operating Systems
:published_at: 2016-08-24
:hp-tags: jHiccup, Latency, Sleep, Operating System, Windows, OSX, Ubuntu, Scientific Linux, Real-Time, Control

//NOTE: Keep X in Mind
//image::cover-image.jpg[150, 250, link="http://docs.hebi.us"]
//video::KCylB780zSM[youtube]

// Writer's guide
// http://asciidoctor.org/docs/asciidoc-writers-guide/#links-and-images
// https://github.com/HubPress/hubpress.io/blob/master/Writers_Guide.adoc

== Introduction

// Arbitrary requirements are bad. Not much information out there. Planning on blog series about various aspects.

Latency is an important topic for many applications that is unfortunately often misunderstood. I've spent many hours looking for information on the latency of various components, but have found very little informative data out there. Most benchmarks focus on the maximum throughput and either completely neglect latency, or measure it incorrectly.

My own background is in robotics research. I've spent several years as a staff software engineer at the Robotics Institute at Carnegie Mellon University and am a co-founder of HEBI Robotics, a startup developing modular robotic components. We've worked on many different types of robots, including collaborative arms, wheeled robots, walking robots and snake robots.

Robots are controlled in _real-time_, which means that a command gets executed within a _deadline_ (fixed period of time). There are _hard real-time_ systems that must never exceed their deadline, and _soft real-time_ systems that are able to occasionally handle reasonable outliers. Missing deadlines can result in unwanted motions and 'jerky' behavior.
 
Although there is a lot of information on the theoretical definition of these terms, it can be challenging to determine good deadlines for practical applications. This is especially true for research environments that build unique mechanisms.

Unfortunately, the control scheme is a systemic concern that can impact the entire system architecture. This can be very difficult to change later on, so there is a tendency towards high requirements with regards to determinism. However, requirements that are too high can result in significant development efforts that may not yield any benefits in the real world.

// There is also a lot of folklore about the reliablity and performance of various components. I have been in countless debates where people have tried to convince me that a proposed system has no chance of working, not knowing that it had already been implemented and been running without issues. This is partly because there is very little useful data on latency out there. Most benchmarks focus only on throughput, and some that do include latency were measured incorrectly.

Over a series of blog posts, I'll try to share some my own experiences and data sets. Today's post will focus on operating systems.

=== Measuring Latency

// Data is not normally distributed. What is a better way to look at latency? What are tools that do this? How does jHiccup work? Gil Tene mentions coordinated omission, but that is less of a problem for request/response systems.

The first important realization when looking at latency is that data does not follow a Gaussian distribution. I have seen many data sets where the worst observed case was more than 1000 standard deviations away from the mean. Looking at only the mean and standard deviation tends to provide an extremely optimistic view that can be misleading.

A better way to look at latency is via histograms and percentile plots, e.g., "99.9% of measurements were below X ms". There are already several good resources about recording latency out there, so I won't go into detail. Please refer to link:http://psy-lob-saw.blogspot.com/2015/02/hdrhistogram-better-latency-capture.html[HdrHistogram: A better latency capture method] or link:https://youtu.be/lJ8ydIuPFeU[How NOT to Measure Latency] for more information.

//link:http://latencytipoftheday.blogspot.com/[Gil Tene]
//video::lJ8ydIuPFeU[youtube]

link:https://www.azul.com[Azul Systems] sells products targeted at latency sensitive applications and they have created a variety of useful tools to measure latency. link:https://www.azul.com/jhiccup/[jHiccup] is a tool that allows us to measure and record system level hiccups (~jitter).It measures the time for _sleep(1ms)_ and records the delta to the fastest previously recorded sample. For example, if the fastest sample was 1ms, but it took 3ms to wake up, it will record a 2ms 'hiccup'. Hiccups can be caused by a large number of reasons, including scheduling, paging, indexing, and many more. By running it on an idle system, we can measure the best case scenario. jHiccup uses link:https://github.com/HdrHistogram/HdrHistogram[HdrHistogram] for the recording of samples.

Lastly, I'm using link:https://github.com/ennerf/HdrHistogramVisualizer[HdrHistogramVisualizer] which is a tool that I've written for visualizing the recorded data.

== Operating Systems

//jHiccup is a great tool developed by Azul Systems that allows us to measure and record hiccups ('jitter')  at the OS level. These can be caused by a large number of reasons, including swap, indexing tasks, and many more. By running it on an idle system, we can measure the best case scenario.
 
The operating system is the base of everything. No matter how amazing the software stack is, the system is fundamentally bound by the capabilities of the OS, it's scheduler, and the overall load on the system. Before you start optimizing your own software, you should make sure that your goal is actually achievable on the underlying platform.

There is a trade-off between responding in a timely manner and overall performance, battery life, and many other concerns. As a result, the major consumer operating systems don't guarantee to meet hard deadlines and can theoretically have arbitrarily long pauses.

However, commodity operating systems can significantly ease development. It is worth evaluating their actual performance before prematurely dismissing them for real-time control systems. Even though there may not be any theoretical guarantees, the practical differences are often not noticeable.

=== Benchmark Setup

I've setup two standard desktop computers, one for Windows/Linux and one for Mac tests.

//[width="100%",frame="topbot"]
//|=========
//|                 | |*CPU* |*RAM*
//|*Windows/Linux* |Gigabyte Brix Bxi7-4770R |i7-4770R @ 3.2 GHz |16 GB 1600 MHz DDR3
//|*Mac* |Mac Mini 2014 |i7-3720QM @ 2.6 GHz |16 GB 1600 MHz DDR3
//|=========

* Gigabyte Brix BXi7-4770R, i7-4770R @ 3.2 GHz, 16 GB 1600 MHz DDR3
* Mac Mini 2014, i7-3720QM @ 2.6 GHz, 16 GB 1600 MHz DDR3

NOTE: When doing latency tests on Windows you should be aware of the system timer. It has variable timer intervals that range from 0.5ms to 15.6ms. By calling _timeBeginPeriod_ and _endTimePeriod_ applications can notify the OS whenever they need a higher resolution. The timer interrupt is a global resource that gets set to the lowest interrupt interval requested by any application. For example, watching a video in Chrome requests a timer interrupt interval of 0.5ms. A lower period results in a more responsive system at the cost of overall throughput and battery life. link:https://vvvv.org/contribution/windows-system-timer-tool[System Timer Tool] is a little utility that let's you view the current state. Calling Java's _Thread.sleep()_ with a value of below 10ms automatically requests a 1ms timer interval.

NOTE: TODO: Confirm that system timer was set to 1ms for test.

=== Windows / Mac / Linux

Let's first look at the performance of consumer operating systems: Windows, Mac and Linux. Each test started off with a clean install for each OS. The only two modifications to the stock installation were to disable sleep mode and to install JDK8 (update 101) to run jHiccup. I then started the test, unplugged all external cables and let the computer sit 'idle' for >24 hours. The actual OS versions were,

//[width="100%",frame="topbot"]
//|=========
//| *OS* |*Version* 
//|*Windows* | Windows 10 Enterprise, version 1511 (OS build: 10586.545)
//|*Mac* | OS X, version 10.9.5
//|*Linux* | Ubuntu 16.04 Desktop, kernel 4.4.0-31-generic
//|*RT Linux* |  Scientific Linux 6.6, kernel 3.10.0-327.rt56.194.el6rt.x86_64
//|=========

* Windows 10 Enterprise, version 1511 (OS build: 10586.545)
* OS X, version 10.9.5
* Ubuntu 16.04 Desktop, kernel 4.4.0-31-generic

Each image below contains two charts. The top section shows the worst hiccup that occured within a given interval window, i.e., the first data point shows the worst hiccup within the first 3 minutes and the next data point shows the worst hiccup within the next 3 minutes. The bottom chart shows the percentiles of all measurements across the entire duration. Each 24 hour data set contains roughly 70-80 million samples.

// 24 hour plot: -/+ 20 min on each side to avoid start/stop noise => sec 1200 to 87600 in aggregate 180 intervals
image::os/osx-win-ubuntu_24h.png[osx windows ubuntu latency over 24 hours, link="/images/os/osx-win-ubuntu_24h.png"]

Up to the 90th percentile all three systems respond relatively similarly, but there are significant differences at the higher percentiles. There also seems to have been a period of increased system activity on OSX after 7 hours. The chart below shows a zoomed in view of a 10 minute time period starting at 10 hours.

// 10 min plot: 36005 to 36590 in aggregate 1 intervals
image::os/osx-win-ubuntu_10m.png[osx windows ubuntu latency over 10 minutes, link="/images/os/osx-win-ubuntu_10m.png"]

Overall Ubuntu 16.04 seems to have very little hiccups compared to Windows and OSX.

=== Real Time Linux

Now that we have a better understanding of how traditional systems without tuning perform, let's take a look at the performance of Linux with a real-time kernel. The rt kernel (PREEMPT_RT patch) can preempt lower priority tasks, which results in worse overall performance, but more deterministic behavior with respect to latency.

I've chosen Scientific Linux 6 because of it's support for link:https://access.redhat.com/products/red-hat-enterprise-mrg-realtime[Red Hat(R) Enterprise MRG Realtime(R)]. You can download the  link:http://ftp.scientificlinux.org/linux/scientific/[ISO] and find instructions for installing MRG Realtime link:http://linux.web.cern.ch/linux/mrg/[here]. The version I've tested was,

* Scientific Linux 6.6, kernel 3.10.0-327.rt56.194.el6rt.x86_64

Note that there is a huge number of tuning options that may improve the performance of your application. There are various tuning guides that can provide more information, e.g., Red Hat's link:http://linux.web.cern.ch/linux/mrg/2.3/Red_Hat_Enterprise_MRG-2-Realtime_Tuning_Guide-en-US.pdf[MRG Realtime Tuning Guide]. I'm not very familiar with tuning systems at this level, so I've only applied the following small list of changes.

* _/boot/grub/menu.lst_ => _transparent_hugepage=never_
* _/etc/sysctl.conf_ => _vm.swappiness=0_ 
* _/etc/inittab_ => _id:3:initdefault_ (no GUI)
* _chkconfig --level 0123456 cpuspeed off_

The process priority was set to 98, which is the highest priority available for real-time threads. I'd advise consulting  
link:https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_MRG/2/html/Realtime_Tuning_Guide/chap-Realtime-Specific_Tuning.html#Setting_scheduler_priorities[scheduler priorities] before deciding on priorities for tasks that actually use cpu time.

[source,shell]
---------------------------------------------------------------------
# find process id
pid=$(pgrep -f "[j]Hiccup.jar")

# show current priority
echo $(chrt -p $pid)

# set priority
sudo chrt -p 98 $pid
---------------------------------------------------------------------

Below is a comparison of the two Linux variants. 24 hours with 3 minute intervals,

image::os/ubuntu-scl_24h.png[ubuntu scientific linux latency over 24 hours, link="/images/os/ubuntu-scl_24h.png"]

and 10 minutes at 1 second intervals.

image::os/ubuntu-scl_10m.png[ubuntu scientific linux latency over 10 minutes, link="/images/os/ubuntu-scl_10m.png"]

I've also added the 24 hour chart for only the real-time variant to provide a better scale. Note that this resolution is getting close to what we can measure and record.

image::os/scl_24h.png[scientific linux latency over 24 hours, link="/images/os/scl_24h.png"]

==	Final Notes

//It's easy to do 100 Hz control in just about any OS. 1KHz hard real-time requires lots of tuning. Rates are highly dependent on the application. Hard to generalize.
I've tried to provide a basic idea of the out of the box performance of various off the shelf operating systems. Actual requirements are heavily dependent on the specific use case, so it's impossible to make a general recommendation. While some safety critical applications may require a real time operating system (RTOS), others may run perfectly fine on commodity systems. My goal is to provide an overview that allows you to make a more informed decision about suitable platform after you've established your requirements. I'm planning on doing a future post on actually figuring out sane requirements.

Note that these are not your only options. There are many different link:https://en.wikipedia.org/wiki/Comparison_of_real-time_operating_systems[RTOS] out there. There are even real-time extensions for Windows, e.g., link:http://www.tenasys.com/overview-ifw[INtime] or link:http://kithara.com/en/products/realtime-suite[Kithara]. However, since integrating such systems can be very expensive or time consuming, I'd recommend going with simple and community supported solutions unless necessary.

=== Latency is not Gaussian

Finally, I'd like to stress again that latency practically never follows a Gaussian distribution. The table for these data sets is below.

[width="80%"]
|========
| |*Samples* |*Mean* |*StdDev* |*Max* | *(max-mean) /stddev*
|*Windows 10* |80,304,595 |0.55 ms |0.37 |17.17 ms |44.9
|*OSX 10.9.5*     |65,282,969 |0.32 ms |0.03 |12.65 ms |411
|*Ubuntu 16.04*   |78,039,162 |0.10 ms |0.01 |3.03 ms |293
|*Scientific Linux 6.6-rt*   |79.753.643 |0.08 ms|0.01 |0.15 ms |7
|========

The max for OSX is more than 400 standard deviations away from the mean. Using only mean/stddev for any sort of latency comparison can produce unwanted results. Aside from giving little to no information about the higher percentiles, there are many cases where systems with seemingly better values exhibit worse actual performance.



